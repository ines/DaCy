{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitea838f35b129458e9787f325adb7b1c3",
   "display_name": "Python 3.9.2 64-bit ('local')"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Wrapping a fine-tuned Transformer\n",
    "\n",
    "---\n",
    "This tutorial will briefly take your through how to wrap an already trained transformer for text classification in a spacy pipeline. For this example we will use DaNLP's BertTone as an example. However do note that this approach also works using models directly from Huggingface's model hub.\n",
    "\n",
    "Before we start let us make sure everything is installed:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/KennethEnevoldsen/DaCy"
   ]
  },
  {
   "source": [
    "## Wrapping a Huggingface model\n",
    "---\n",
    "It is now easy to wrap a huggingface model it can be done simply in one line of code:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.subclasses import add_huggingface_model\n",
    "nlp = spacy.blank(\"da\")  # replace with your desired pipeline\n",
    "nlp = add_huggingface_model(nlp, \n",
    "                      download_name=\"pin/senda\", # the model name on the huggingface hub\n",
    "                      doc_extention=\"senda_trf_data\", # the doc extention transformer data e.g. including wordpieces\n",
    "                      model_name=\"senda\",  # the name of the model in the pipeline\n",
    "                      category=\"polarity\", # the category type it predicts\n",
    "                      labels=[\"negative\", \"neutral\", \"positive\"], # possible outcome labels\n",
    "                      )"
   ]
  },
  {
   "source": [
    "## Step by step\n",
    "---\n",
    "However, one might want to look a bit more into it so let is go through step by step. First of all let is start with the setup. We will here use a downloaded huggingface transformer from DaNLP.\n",
    "\n",
    "### Setup\n",
    "Let's start of by downloading the model as be sure that it can be loaded in using Huggingface transformers:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.download import download_model as danlp_download\n",
    "from danlp.download import _unzip_process_func\n",
    "from danlp.download import DEFAULT_CACHE_DIR as DANLP_DIR\n",
    "from transformers import AutoModelForSequenceClassification, BertTokenizer\n",
    "\n",
    "# downloading model and setting a path to its location\n",
    "path_sub = danlp_download(\n",
    "    \"bert.polarity\", DANLP_DIR, process_func=_unzip_process_func, verbose=True\n",
    ")\n",
    "path_sub = os.path.join(path_sub, \"bert.pol.v0.0.1\")\n",
    "\n",
    "# loading it in with transformers\n",
    "berttone = AutoModelForSequenceClassification.from_pretrained(path_sub, num_labels=3)"
   ]
  },
  {
   "source": [
    "Assuming this works you are ready to move on. However if you were to do this on your own model I would also test that the forward pass works as intended.\n",
    "\n",
    "## Wrapping the Model\n",
    "\n",
    "---\n",
    "\n",
    "Now we will start wrapping the model. DaCy provides good utility function for doing precisely this without make more changes than necessary to the transformer class from SpaCy. This should allow you to use the extensive documentation by SpaCy while working with this code.\n",
    "\n",
    "This is utilizes SpaCy config system, which might take a bit of getting used to initially, but it is quite worth the effort. For now I will walk you through it:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.subclasses import ClassificationTransformer, install_classification_extensions\n",
    "\n",
    "labels=[\"positive\", \"neutral\", \"negative\"]\n",
    "doc_extension = \"berttone_pol_trf_data\"\n",
    "category = \"polarity\"\n",
    "\n",
    "config = {\n",
    "    \"doc_extention_attribute\": doc_extension,\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"dacy.ClassificationTransformerModel.v1\",\n",
    "        \"name\": path_sub,\n",
    "        \"num_labels\": len(labels),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# add the relevant extentsion to the doc\n",
    "install_classification_extensions(\n",
    "    category=category, labels=labels, doc_extention=doc_extension\n",
    ")"
   ]
  },
  {
   "source": [
    "The config files is an extention of the `Transformers` config in SpaCy, but you will note a few changes:\n",
    "\n",
    "- 1) `doc_extention_attribute`: This is to make sure that the doc extension can be customized. The doc extentsion is how you fetch data relevant to your model. The SpaCy transformer uses the `trf_data`, but we don't want to overwrite this in case we are using multiple transformers.\n",
    "- 2) `num_labels`: The number of labels. This is an argument passed forward when loading the model. Without this the Huggingface transformers package will raise an error (at least for cases where `num_labels` isn't 2)\n",
    "\n",
    "Some might also ask regarding the `name`, but this simply specifies the name of the model. You could potentially change this out for any sequence classification model on Huggingfaces model hub. Lastly the `install_classification_extensions` adds the getter function for the model. Here it would for instance add `doc._.polarity` for extracting the label of the model as well as a `doc._.polarity_prop` for extracting the polarity probabilities for each class.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adding it to the NLP pipeline\n",
    "---\n",
    "\n",
    "Now it can simply add it to the pipeline using the `add_pipe`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<thinc.model.Model at 0x15e411840>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"da\") # dummy nlp\n",
    "\n",
    "clf_transformer = nlp.add_pipe(\n",
    "    \"classification_transformer\", name=\"berttone\", config=config\n",
    ")\n",
    "clf_transformer.model.initialize()\n"
   ]
  },
  {
   "source": [
    "## Final Test\n",
    "\n",
    "--- \n",
    "\n",
    "We can then finish off with a final test to see if everything works as intended:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "negative\n{'prop': array([0.002, 0.008, 0.99 ], dtype=float32), 'labels': ['positive', 'neutral', 'negative']}\npositive\n{'prop': array([0.854, 0.146, 0.001], dtype=float32), 'labels': ['positive', 'neutral', 'negative']}\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Analysen viser, at økonomien bliver forfærdelig dårlig\", \n",
    "         \"Jeg tror alligvel, det bliver godt\"]\n",
    "\n",
    "docs = nlp.pipe(texts)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc._.polarity)\n",
    "    print(doc._.polarity_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TransformerData(wordpieces=WordpieceBatch(strings=[['[CLS]', 'jeg', 'tror', 'alli', '##g', '##vel', ',', 'det', 'bliver', 'godt', '[SEP]']], input_ids=array([[    2,   102,  1421,  9682, 31704,  1041,   883,    49,   352,\n",
       "          380,     3]]), attention_mask=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), lengths=[11], token_type_ids=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), tensors=[array([[ 3.3191876,  1.5510517, -4.0984917]], dtype=float32)], align=Ragged(data=array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int32), lengths=array([1, 1, 3, 1, 1, 1, 1], dtype=int32), data_shape=(-1,), cumsums=None))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# we can also examine the wordpieces used using (and see the entire TransformersData)\n",
    "\n",
    "doc._.berttone_pol_trf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}